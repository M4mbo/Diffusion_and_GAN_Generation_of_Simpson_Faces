{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M4mbo/Generative_Adversarial_Network_on_Simpsons_Faces/blob/main/MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init(m):\n",
        "    \"\"\"Reinitialize model weights. GAN authors recommend them to be sampled from N(0,0.2)\"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "n_IADw4VFGnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Reshape(nn.Module):\n",
        "  \"\"\"A custom reshape layer.\"\"\"\n",
        "  def __init__(self, shape):\n",
        "    super(Reshape, self).__init__()\n",
        "    self.shape = shape\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.view(*self.shape)"
      ],
      "metadata": {
        "id": "eS1u0I7RGwTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Generator model\"\"\"\n",
        "  def __init__(self, Z):\n",
        "    super(Generator, self).__init__()\n",
        "    self.Z = Z\n",
        "\n",
        "    self.gen_model = nn.Sequential(\n",
        "\n",
        "        nn.Linear(self.Z, 1024*8*8),\n",
        "        nn.BatchNorm1d(1024*8*8),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        Reshape((-1, 1024, 8, 8)),\n",
        "\n",
        "        nn.ConvTranspose2d(1024, 512, 5, 2, 1, 0),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(512, 256, 5, 2, 2, 0),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(256, 128, 5, 2, 2, 0),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(128, 64, 5, 2, 2, 1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(64, 3, 5, 1, 1),\n",
        "\n",
        "    )\n",
        "\n",
        "  def forward(self, noise):\n",
        "\n",
        "    x = self.gen_model(noise)\n",
        "\n",
        "    x = F.tanh(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "Uq44AehZFIBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  \"\"\"Discriminator model\"\"\"\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.disc_model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(in_channels=256, out_channels=512, kernel_size=5, stride=1, padding=2),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=5, stride=2, padding=2),\n",
        "        nn.BatchNorm2d(1024),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "\n",
        "    )\n",
        "    self.linearization = nn.Sequential(\n",
        "\n",
        "        nn.Flatten(1,-1),\n",
        "        nn.Linear(1024*8*8, 1)\n",
        "\n",
        "    )\n",
        "\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    x = self.disc_model(x)\n",
        "\n",
        "    x = self.linearization(x)\n",
        "\n",
        "    x = self.sigmoid(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "mFi4jxA2FYLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(generator_losses, discriminator_losses):\n",
        "\n",
        "    # Plot Discriminator Training Loss\n",
        "    plt.plot(discriminator_losses, label='Discriminator Training Loss', color='blue')\n",
        "    plt.plot(generator_losses, label='Generator Training Loss', color='red')\n",
        "\n",
        "    # Set the x-axis and y-axis labels\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Set the title and legend\n",
        "    plt.title('Training Losses')\n",
        "    plt.legend()\n",
        "\n",
        "    # Show the grid\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JN-GAZDCFePe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to display results\n",
        "def display_image_grid(images, num_rows, num_cols, title_text):\n",
        "\n",
        "  fig = plt.figure(figsize=(num_cols*3., num_rows*3.), )\n",
        "  grid = ImageGrid(fig, 111, nrows_ncols=(num_rows, num_cols), axes_pad=0.15)\n",
        "\n",
        "  for ax, im in zip(grid, images):\n",
        "    if im.size(0) == 1:\n",
        "      if im.dtype == torch.float32 or im.dtype == torch.float64:\n",
        "        ax.imshow(np.clip(im.permute(1,2,0).numpy(), 0, 1), cmap = 'gray')\n",
        "      else:\n",
        "        ax.imshow(np.clip(im.permute(1,2,0).numpy(), 0, 255), cmap = 'gray')\n",
        "    else:\n",
        "      if im.dtype == torch.float32 or im.dtype == torch.float64:\n",
        "        ax.imshow(np.clip(im.permute(1,2,0).numpy(), 0, 1))\n",
        "      else:\n",
        "        ax.imshow(np.clip(im.permute(1,2,0).numpy(), 0, 255))\n",
        "\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "  plt.suptitle(title_text, fontsize=20)\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "gSAlbFTwGBUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "Z = 100\n",
        "# Visualization Purposes\n",
        "sample_noise = torch.randn(10, Z).to(device)"
      ],
      "metadata": {
        "id": "BZDOP4ULGRPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_GAN(EPOCHS, lrg, lrd, discriminator, generator):\n",
        "\n",
        "  real_label = 1\n",
        "  fake_label = 0\n",
        "\n",
        "  discriminator.train()\n",
        "  generator.train()\n",
        "\n",
        "  writer = SummaryWriter(\"runs/lr_1\")\n",
        "\n",
        "  # Defining the optimizer and loss function here\n",
        "  optimizer_gen = torch.optim.Adam(generator.parameters(), lr=lrg, betas=(0.5, 0.999))\n",
        "  optimizer_disc = torch.optim.Adam(discriminator.parameters(), lr=lrd, betas=(0.5, 0.999))\n",
        "\n",
        "  lr_scheduler1 = optim.lr_scheduler.ExponentialLR(optimizer_disc, gamma=0.99, verbose=True)\n",
        "  lr_scheduler2 = optim.lr_scheduler.ExponentialLR(optimizer_gen, gamma=0.99, verbose=True)\n",
        "\n",
        "  loss_fn = nn.BCELoss()\n",
        "\n",
        "  generator_losses = []\n",
        "  discriminator_losses = []\n",
        "\n",
        "  generated_images = []\n",
        "\n",
        "  ## Training\n",
        "  for i in range(1,EPOCHS+1):\n",
        "    pbar = tqdm(train_loader)\n",
        "\n",
        "    total_gen_loss = 0.0\n",
        "    total_disc_loss = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    for b, batch in enumerate(pbar):\n",
        "      # Every data instance is an input + label pair. We don't need the label\n",
        "      inputs = batch\n",
        "      inputs = inputs.to(device)\n",
        "\n",
        "      inputs = (inputs - 0.5) * 2 # setting data range to [-1,1]\n",
        "\n",
        "      ############################\n",
        "      # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "      ###########################\n",
        "\n",
        "      ## Train with all-real batch\n",
        "      discriminator.zero_grad()\n",
        "      # Set up labels\n",
        "      label = torch.full((inputs.shape[0],1), real_label, dtype=torch.float, device=device)\n",
        "      # Forward pass real batch through D\n",
        "      output_real = discriminator(inputs)\n",
        "      # Calculate loss on all-real batch\n",
        "      errD_real = loss_fn(output_real, label)\n",
        "      # Calculate gradients for D in backward pass\n",
        "      errD_real.backward()\n",
        "      D_x = output_real.mean().item()\n",
        "\n",
        "      ## Train with all-fake batch\n",
        "      noise = torch.randn(inputs.shape[0], Z).to(device)\n",
        "      # Generate fake image batch with G\n",
        "      fake = generator(noise)\n",
        "      label.fill_(fake_label)\n",
        "      # Classify all fake batch with D\n",
        "      output_fake = discriminator(fake.detach())\n",
        "      # Calculate D's loss on the all-fake batch\n",
        "      errD_fake = loss_fn(output_fake, label)\n",
        "      # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "      errD_fake.backward()\n",
        "      D_G_z1 = output_fake.mean().item()\n",
        "      # Compute error of D as sum over the fake and the real batches\n",
        "      errD = errD_real + errD_fake\n",
        "      # Update D\n",
        "      optimizer_disc.step()\n",
        "      ############################\n",
        "      # (2) Update G network: maximize log(D(G(z)))\n",
        "      ###########################\n",
        "      generator.zero_grad()\n",
        "      label.fill_(real_label)  # fake labels are real for generator cost\n",
        "      # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "      output_fake = discriminator(fake)\n",
        "      # Calculate G's loss based on this output\n",
        "      errG = loss_fn(output_fake, label)\n",
        "      # Calculate gradients for G\n",
        "      errG.backward()\n",
        "      D_G_z2 = output_fake.mean().item()\n",
        "      # Update G\n",
        "      optimizer_gen.step()\n",
        "\n",
        "      total_gen_loss += errG.item()\n",
        "      total_disc_loss += errD.item()\n",
        "      num_samples += inputs.size(0)\n",
        "\n",
        "      pbar.set_description(f\"Epoch {i}/{EPOCHS}: \")\n",
        "      pbar.set_postfix({\"generator_loss\": errG.item(), \"discriminator_loss\": errD.item(), \"D(x)\": D_x, \"D(G(z1))\": D_G_z1, \"D(G(z2))\": D_G_z2})\n",
        "\n",
        "    generator_losses.append(total_gen_loss / num_samples)\n",
        "    discriminator_losses.append(total_disc_loss / num_samples)\n",
        "\n",
        "\n",
        "    # Visualization of validation images\n",
        "    generations = generator(sample_noise).cpu()\n",
        "    generations = (generations + 1) / 2   #[0,1]\n",
        "    generations = (generations * 255).clamp(0, 255).to(torch.uint8)\n",
        "    generated_images.append(generations)\n",
        "    display_image_grid(generations, 1, 10, f\"Generated images at epoch {i}\")\n",
        "\n",
        "    lr_scheduler1.step()\n",
        "    lr_scheduler2.step()\n",
        "\n",
        "  return generator_losses, discriminator_losses, generated_images\n"
      ],
      "metadata": {
        "id": "_lcfWYuZGFYI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}